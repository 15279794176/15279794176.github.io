<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>文章</title>
    <link href="/2024/03/25/%E6%96%87%E7%AB%A0/"/>
    <url>/2024/03/25/%E6%96%87%E7%AB%A0/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title></title>
    <link href="/2024/03/25/hello-world/"/>
    <url>/2024/03/25/hello-world/</url>
    
    <content type="html"><![CDATA[<p>%%  清空环境变量<br>warning off             % 关闭报警信息<br>close all               % 关闭开启的图窗<br>clear                   % 清空变量<br>clc                     % 清空命令行</p><p>%%  导入数据<br>res &#x3D; xlsread(‘数据集.xlsx’);</p><p>%%通过随机排列 (randperm) 对数据进行打乱，然后分割成训练集（前70行）和测试集（剩余行）。<br>temp &#x3D; randperm(92);<br>P_train &#x3D; res(temp(1: 70), 1: 7)’;<br>T_train &#x3D; res(temp(1: 70), 8)’;<br>M &#x3D; size(P_train, 2);</p><p>P_test &#x3D; res(temp(71: end), 1: 7)’;<br>T_test &#x3D; res(temp(71: end), 8)’;<br>N &#x3D; size(P_test, 2);</p><p>%%  数据归一化<br>[p_train, ps_input] &#x3D; mapminmax(P_train, 0, 1);<br>p_test &#x3D; mapminmax(‘apply’, P_test, ps_input);</p><p>[t_train, ps_output] &#x3D; mapminmax(T_train, 0, 1);<br>t_test &#x3D; mapminmax(‘apply’, T_test, ps_output);</p><p>%%  newff是MATLAB中用于创建多层前馈神经网络的函数。<br>%%  p_train代表训练集的输入数据，即特征向量。在神经网络训练过程中，这些输入数据会被用来学习权重和偏置，以便网络能对输入产生期望的输出。<br>%%  t_train代表训练集的目标输出数据，对于回归问题，这通常是连续值；对于分类问题，则通常是类别标签的编码形式。<br>%%  5指定了隐藏层的神经元数量。在这个例子中，创建的神经网络有一个隐藏层，该隐藏层包含5个神经元。输入层的大小会自动根据p_train的列数确定，输出层的大小则根据t_train的列数确定。如果t_train是标量（单输出），输出层会有1个神经元；如果是向量（多输出），输出层神经元的数量将与向量的维度相匹配。<br>net &#x3D; newff(p_train, t_train, 5);</p><p>%%  设置训练参数<br>net.trainParam.epochs &#x3D; 1000;     % 迭代次数<br>net.trainParam.goal &#x3D; 1e-6;       % 误差阈值<br>net.trainParam.lr &#x3D; 0.01;         % 学习率为0.01，学习率决定了在每一步训练中，根据梯度下降法更新权重和偏置时的步长。较大的学习率可以让网络快速收敛，但可能造成震荡或越过最优解；较小的学习率可以使训练更加稳定，但可能需要更多的迭代次数才能收敛。0.01是一个相对常见的初始学习率选择，适用于很多情况，但具体最佳值往往需要根据实际问题和数据调整。</p><p>%%  训练网络<br>net&#x3D; train(net, p_train, t_train);</p><p>%%  仿真测试<br>t_sim1 &#x3D; sim(net, p_train);<br>t_sim2 &#x3D; sim(net, p_test);</p><p>%%  数据反归一化<br>T_sim1 &#x3D; mapminmax(‘reverse’, t_sim1, ps_output);<br>T_sim2 &#x3D; mapminmax(‘reverse’, t_sim2, ps_output);</p><p>%%  均方根误差<br>error1 &#x3D; sqrt(sum((T_sim1 - T_train).^2) .&#x2F; M);<br>error2 &#x3D; sqrt(sum((T_sim2 - T_test ).^2) .&#x2F; N);</p><p>%%  绘图<br>figure<br>plot(1: M, T_train, ‘r-*’, 1: M, T_sim1, ‘b-o’, ‘LineWidth’, 1)<br>legend(‘真实值’, ‘预测值’)<br>xlabel(‘预测样本’)<br>ylabel(‘预测结果’)<br>string &#x3D; {‘训练集预测结果对比’; [‘RMSE&#x3D;’ num2str(error1)]};<br>title(string)<br>xlim([1, M])<br>grid</p><p>figure<br>plot(1: N, T_test, ‘r-*’, 1: N, T_sim2, ‘b-o’, ‘LineWidth’, 1)<br>legend(‘真实值’, ‘预测值’)<br>xlabel(‘预测样本’)<br>ylabel(‘预测结果’)<br>string &#x3D; {‘测试集预测结果对比’; [‘RMSE&#x3D;’ num2str(error2)]};<br>title(string)<br>xlim([1, N])<br>grid</p><p>%%  相关指标计算<br>% R2<br>R1 &#x3D; 1 - norm(T_train - T_sim1)^2 &#x2F; norm(T_train - mean(T_train))^2;<br>R2 &#x3D; 1 - norm(T_test  - T_sim2)^2 &#x2F; norm(T_test  - mean(T_test ))^2;</p><p>disp([‘训练集数据的R2为：’, num2str(R1)])<br>disp([‘测试集数据的R2为：’, num2str(R2)])</p><p>% MAE<br>mae1 &#x3D; sum(abs(T_sim1 - T_train)) .&#x2F; M ;<br>mae2 &#x3D; sum(abs(T_sim2 - T_test )) .&#x2F; N ;</p><p>disp([‘训练集数据的MAE为：’, num2str(mae1)])<br>disp([‘测试集数据的MAE为：’, num2str(mae2)])</p><p>% MBE<br>mbe1 &#x3D; sum(T_sim1 - T_train) .&#x2F; M ;<br>mbe2 &#x3D; sum(T_sim2 - T_test ) .&#x2F; N ;</p><p>disp([‘训练集数据的MBE为：’, num2str(mbe1)])<br>disp([‘测试集数据的MBE为：’, num2str(mbe2)])</p><p>%%  绘制散点图<br>sz &#x3D; 25;<br>c &#x3D; ‘b’;</p><p>figure<br>scatter(T_train, T_sim1, sz, c)<br>hold on<br>plot(xlim, ylim, ‘–k’)<br>xlabel(‘训练集真实值’);<br>ylabel(‘训练集预测值’);<br>xlim([min(T_train) max(T_train)])<br>ylim([min(T_sim1) max(T_sim1)])<br>title(‘训练集预测值 vs. 训练集真实值’)</p><p>figure<br>scatter(T_test, T_sim2, sz, c)<br>hold on<br>plot(xlim, ylim, ‘–k’)<br>xlabel(‘测试集真实值’);<br>ylabel(‘测试集预测值’);<br>xlim([min(T_test) max(T_test)])<br>ylim([min(T_sim2) max(T_sim2)])<br>title(‘测试集预测值 vs. 测试集真实值’)</p><p>%%  保存所需变量<br>save net.mat net<br>save ps_input.mat ps_input<br>save ps_output.mat ps_output</p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
